{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport math\nfrom glob import glob\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport zipfile\nimport urllib.request\nimport os.path\nfrom IPython.display import display\nfrom math import ceil\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms.functional import to_pil_image\nfrom matplotlib import colormaps\nimport PIL\nimport torchvision.ops as ops","metadata":{"id":"1V_MxajUh3-p","execution":{"iopub.status.busy":"2023-12-02T21:37:21.318249Z","iopub.execute_input":"2023-12-02T21:37:21.318542Z","iopub.status.idle":"2023-12-02T21:37:25.284582Z","shell.execute_reply.started":"2023-12-02T21:37:21.318514Z","shell.execute_reply":"2023-12-02T21:37:25.283529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataPreprocessing(Dataset):\n\n    classEncoding = {\n        'Atelectasis': torch.FloatTensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        'Consolidation': torch.FloatTensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        'Infiltration': torch.FloatTensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        'Pneumothorax': torch.FloatTensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        'Edema': torch.FloatTensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        'Emphysema': torch.FloatTensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        'Fibrosis': torch.FloatTensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n        'Effusion': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n        'Pneumonia': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n        'Pleural_Thickening': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n        'Cardiomegaly': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n        'Nodule': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n        'Hernia': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n        'Mass': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n        'No Finding': torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n    }\n\n    def __init__(self,classEncoding=classEncoding, max_samples=None):\n        self.image_names = []\n        self.labels = []\n        #The original code opened the labels file from the zip file. I upload the labels file manually to the directory and open it manually here\n        f = open('/kaggle/input/data/Data_Entry_2017.csv','r')\n        #with zipfile.ZipFile('images_samples2.zip', 'r') as z:\n          #f = z.open('sample_labels_504.csv','r')\n        title = True\n        for line in f:\n          if (title):\n            title = False\n            continue\n          items = str(line).split(\",\")\n            \n          #image_name = items[0][2:]\n          image_name = items[0]\n          #print(image_name)\n          image_name = glob(os.path.join(\"/kaggle/input/data/*/*\", image_name))[0]\n          #print(image_name)\n          self.image_names.append(image_name)\n\n          label = items[1]  # list of diseases\n          diseases_list = label.split(\"|\")\n          labelTensor = torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n          for disease in diseases_list:\n            labelTensor = labelTensor.add(classEncoding[disease])\n          self.labels.append(labelTensor)\n\n          if max_samples and len(self.image_names) >= max_samples:\n            break\n\n#Modified code\n    def __getitem__(self, index):\n      #The original code opened the images directly from the zip file. As we already unziped them and we manually changed the current directory\n      #to where they were unziped, we just open the images from the current directory\n      image_path = self.image_names[index]\n      #with zipfile.ZipFile('images_samples2.zip', 'r') as z:\n      #image = Image.open(z.open(image_path,'r')).convert('RGB')\n      image = Image.open(image_path,'r').convert('RGB')\n      normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n      preprocess = transforms.Compose([\n            transforms.Resize(350),\n            transforms.CenterCrop(300),\n            transforms.RandomHorizontalFlip(p=0.2),\n            transforms.ToTensor()\n            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n      image = preprocess(image)\n      return image, self.labels[index]\n\n#Original code\n    # def __getitem__(self, index):\n    #   image_path = self.image_names[index]\n    #   with zipfile.ZipFile('images_samples2.zip', 'r') as z:\n    #     image = Image.open(z.open(image_path,'r')).convert('RGB')\n    #     normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    #     preprocess = transforms.Compose([\n    #         transforms.Resize(256),\n    #         transforms.TenCrop(224),\n    #         transforms.Lambda\n    #         (lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n    #         transforms.Lambda\n    #         (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n    #     ])\n    #     image = preprocess(image)\n    #   return image, self.labels[index]\n\n\n    def __len__(self):\n        return len(self.image_names)\n\n","metadata":{"id":"NXyoU21yv65L","execution":{"iopub.status.busy":"2023-12-02T21:37:30.800486Z","iopub.execute_input":"2023-12-02T21:37:30.801228Z","iopub.status.idle":"2023-12-02T21:37:30.835143Z","shell.execute_reply.started":"2023-12-02T21:37:30.801181Z","shell.execute_reply":"2023-12-02T21:37:30.834336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_score_with_logits(logits, labels):\n  logits = torch.max(logits, 1)[1].data # argmax\n  one_hots = torch.zeros(*labels.size()).cuda()\n  one_hots.scatter_(1, logits.view(-1, 1), 1)\n  scores = (one_hots * labels)\n\n  return scores\n\ndef tile(a, dim, n_tile):\n  init_dim = a.size(dim)\n  repeat_idx = [1] * a.dim()\n  repeat_idx[dim] = n_tile\n  a = a.repeat(*(repeat_idx))\n  order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n  return torch.index_select(a, dim, order_index)\n\ndef get_label_for_image(model, image_path):\n  classes = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis',\n      'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Hernia', 'Mass', 'No Finding']\n  input_image_grey = Image.open(image_path)\n  input_image = input_image_grey.convert('RGB')\n  preprocess = transforms.Compose([transforms.Resize(350),\n                                   transforms.CenterCrop(300),\n                                   transforms.RandomHorizontalFlip(p=0.2),\n                                   transforms.ToTensor()\n                                   #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                   ])\n  input_tensor = preprocess(input_image)\n  input_batch = input_tensor.unsqueeze(0)\n  output = model(input_batch)\n  index_tensor = torch.argmax(output)\n  index = index_tensor.item()\n  return classes[index]","metadata":{"id":"hpdh2jegCzgL","execution":{"iopub.status.busy":"2023-12-02T21:37:31.437212Z","iopub.execute_input":"2023-12-02T21:37:31.437883Z","iopub.status.idle":"2023-12-02T21:37:31.447992Z","shell.execute_reply.started":"2023-12-02T21:37:31.437848Z","shell.execute_reply":"2023-12-02T21:37:31.447057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = [\n    # layer, expand_ratio, channels, repeats, stride, kernel_size\n    [1, 1, 24, 2, 1, 3],\n    [2, 4, 32, 3, 2, 3],\n    [3, 4, 48, 3, 2, 3],\n    [4, 6, 96, 5, 2, 3],\n    [5, 6, 136, 5, 1, 5],\n    [6, 6, 232, 6, 2, 5],\n    [7, 6, 384, 2, 1, 3],\n]","metadata":{"id":"7NMWR8EsCUSL","execution":{"iopub.status.busy":"2023-12-02T21:37:32.112158Z","iopub.execute_input":"2023-12-02T21:37:32.112779Z","iopub.status.idle":"2023-12-02T21:37:32.119938Z","shell.execute_reply.started":"2023-12-02T21:37:32.112735Z","shell.execute_reply":"2023-12-02T21:37:32.118736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBnAct(nn.Module):\n  def __init__(\n      self,in_channels,out_channels,kernel_size=3, stride=1, padding=0, groups=1\n  ):\n    super(ConvBnAct, self).__init__()\n    self.cnn = nn.Conv2d(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=groups,\n        bias=False,\n    )\n    self.bn = nn.BatchNorm2d(out_channels)\n    self.gelu = nn.GELU()\n\n  def forward(self, x):\n    return self.gelu(self.bn(self.cnn(x)))\n\nclass SqueezeExcitation(nn.Module):\n  def __init__(self, in_channels, reduced_dim):\n    super(SqueezeExcitation, self).__init__()\n    self.se = nn.Sequential(\n        nn.AdaptiveAvgPool2d(1),  #Global Average pooling\n        nn.Conv2d(in_channels, reduced_dim, kernel_size=1),\n        nn.GELU(),\n        nn.Conv2d(reduced_dim, in_channels, kernel_size=1),\n        nn.Sigmoid(),\n    )\n\n  def forward(self, x):\n    return x * self.se(x)\n\nclass FusedMBConv(nn.Module):\n  def __init__(\n      self, in_channels, out_channels, kernel_size, stride, expand_ratio, reduction=4\n  ):\n    super(FusedMBConv, self).__init__()\n    hidden_dim = in_channels * expand_ratio\n    reduced_dim = int(in_channels / reduction)\n    padding = (kernel_size - 1) // 2\n\n    self.conv = nn.Sequential(\n        ConvBnAct(\n            in_channels,\n            hidden_dim,\n            kernel_size,\n            stride,\n            padding=padding,\n            groups=1,\n            ),\n        SqueezeExcitation(hidden_dim, reduced_dim),\n        nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.GELU()\n        )\n\n  def forward(self, x):\n    return self.conv(x)\n\nclass MBConvN(nn.Module):\n  def __init__(\n      self, in_channels, out_channels, kernel_size, stride, expand_ratio, reduction=4\n  ):\n    super(MBConvN, self).__init__()\n    hidden_dim = in_channels * expand_ratio\n    self.expand = in_channels != hidden_dim\n    reduced_dim = int(in_channels / reduction)\n    padding = (kernel_size - 1) // 2\n\n    if self.expand:\n      self.expand_conv = ConvBnAct(\n          in_channels, hidden_dim, kernel_size=1, stride=1, padding=1\n      )\n    self.conv = nn.Sequential(\n        ConvBnAct(\n            hidden_dim,\n            hidden_dim,\n            kernel_size,\n            stride,\n            padding,\n            groups=hidden_dim,\n            ),\n        SqueezeExcitation(hidden_dim, reduced_dim),\n        nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.GELU()\n        )\n\n  def forward(self, inputs):\n    x = self.expand_conv(inputs) if self.expand else inputs\n    return self.conv(x)\n\n#efficientnet-b3 (1.2, 1.4, 300, 0.3)\nclass EfficientNet(nn.Module):\n  def __init__(self,width_factor=1.2,depth_factor=1.4,dropout_rate=0.3,num_classes=15):\n    super(EfficientNet, self).__init__()\n    self.width_factor = width_factor\n    self.dropout_rate = dropout_rate\n    last_channels = ceil(1280 * width_factor)\n    self.pool = nn.AdaptiveAvgPool2d(1)\n    self.features = self.create_features(width_factor, last_channels)\n    self.classifier = nn.Sequential(\n        nn.Dropout(dropout_rate),\n        nn.Linear(last_channels,num_classes),\n        nn.Softmax(dim=1)\n    )\n\n  def create_features(self,width_factor, last_channels):\n    channels = 4*ceil(int(32 * width_factor)/4)\n    features = [ConvBnAct(3, channels, 3, stride=2, padding=1)]\n    in_channels = channels\n\n    for layer_num, expand_ratio, channels, repeats, stride, kernel_size in base_model:\n      out_channels = channels\n      layers_repeats = repeats\n      for layer in range(layers_repeats):\n        if layer_num<=3:\n          features.append(\n              FusedMBConv(\n                  in_channels,\n                  out_channels,\n                  kernel_size=kernel_size,\n                  stride=stride if layer==0 else 1,\n                  expand_ratio=expand_ratio\n              )\n          )\n        else:\n          features.append(\n              MBConvN(\n                  in_channels,\n                  out_channels,\n                  kernel_size=kernel_size,\n                  stride=stride if layer==0 else 1,\n                  expand_ratio=expand_ratio\n              )\n          )\n        in_channels = out_channels\n\n    features.append(\n        ConvBnAct(in_channels, last_channels, 1, stride=1, padding=0)\n        )\n    return nn.Sequential(*features)\n\n  def forward(self, x):\n    x = self.pool(self.features(x))\n    return self.classifier(x.view(x.shape[0], -1))","metadata":{"id":"EZRQJWyVJr83","execution":{"iopub.status.busy":"2023-12-02T21:37:33.400035Z","iopub.execute_input":"2023-12-02T21:37:33.400640Z","iopub.status.idle":"2023-12-02T21:37:33.424078Z","shell.execute_reply.started":"2023-12-02T21:37:33.400606Z","shell.execute_reply":"2023-12-02T21:37:33.422998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.5):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets)\n        pi = torch.exp(-BCE_loss)\n        Focal_loss = self.alpha * (1-pi)**self.gamma * BCE_loss\n        return torch.mean(Focal_loss)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T21:37:33.997459Z","iopub.execute_input":"2023-12-02T21:37:33.997827Z","iopub.status.idle":"2023-12-02T21:37:34.004166Z","shell.execute_reply.started":"2023-12-02T21:37:33.997797Z","shell.execute_reply":"2023-12-02T21:37:34.003024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata = DataPreprocessing()\ntrain_set, test_set = random_split(data, [math.ceil(len(data) * 0.8), math.floor(len(data) * 0.2)])\ntrain,validate = random_split(train_set, [math.ceil(len(train_set) * 0.9), math.floor(len(train_set) * 0.1)])\n\ntrainloader = torch.utils.data.DataLoader(train, batch_size=20, shuffle=True, num_workers=5)\nvalidateloader = torch.utils.data.DataLoader(validate, batch_size=20, shuffle=True, num_workers=5)\ntestloader = torch.utils.data.DataLoader(test_set, batch_size=20, shuffle=False)\n\nclasses = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis',\n      'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Hernia', 'Mass', 'No Finding']\n\nmodel = EfficientNet().cuda()\n#model = nn.DataParallel(model).cuda()","metadata":{"id":"IK4DEU9XC3SM","outputId":"cad59282-1889-42c6-9fcb-ef43db737bbe","execution":{"iopub.status.busy":"2023-12-02T21:37:35.913724Z","iopub.execute_input":"2023-12-02T21:37:35.914462Z","iopub.status.idle":"2023-12-02T21:37:41.995942Z","shell.execute_reply.started":"2023-12-02T21:37:35.914432Z","shell.execute_reply":"2023-12-02T21:37:41.994916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nloss_list = []\nepochs_list = []\nval_loss_list = []\n#criterion = nn.BCELoss()\ncriterion = FocalLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.004, momentum=0.88)\n#optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.005)\n\nfor epoch in range(6):\n  epochs_list.append(epoch+1)\n  running_loss = 0.0\n  correct = 0\n  total = 0\n  for i, (images, labels) in enumerate(trainloader, 0): # get the inputs; data is a list of [images, labels]\n    optimizer.zero_grad()\n\n    images = images.cuda()\n\n    #format input\n    n_batches, channels, height, width = images.size()\n    image_batch = torch.autograd.Variable(images.view(-1, channels, height, width)) #640 images: 64 batches contain 10 crops each decomposed into 640 images\n\n    labels = tile(labels, 0, 1).cuda() #duplicate for each crop the label [1,2],[3,4] => [1,2],[1,2],[3,4],[3,4] -> 640 labels\n\n    # forward + backward + optimize\n    outputs = model(image_batch)\n    loss = criterion(outputs, labels.float())\n    loss.backward()\n    optimizer.step()\n\n    # print statistics\n    running_loss += loss.item()\n\n    correct += compute_score_with_logits(outputs, labels).sum()\n    total += labels.size(0)\n\n  print('Epoch: %d, loss: %.3f, Accuracy: %.3f' %\n        (epoch + 1, running_loss, 100 * correct / total))\n  loss_list.append(running_loss)\n  val_correct = 0\n  val_total = 0\n  val_running_loss = 0\n  with torch.no_grad():\n    for i, (images, labels) in enumerate(validateloader, 0):\n      images = images.cuda()\n      n_batches, channels, height, width = images.size()\n      image_batch = torch.autograd.Variable(images.view(-1, channels, height, width))\n      labels = tile(labels, 0, 1).cuda()\n      outputs = model(image_batch)\n      val_correct += compute_score_with_logits(outputs, labels).sum()\n      val_total += labels.size(0)\n      val_loss = criterion(outputs, labels.float())\n      val_running_loss += val_loss.item()\n\n  print('Accuracy on validation set: %.3f' % (100 * val_correct / val_total))\n  val_loss_list.append(val_running_loss*9)\n\nprint('Finished Training')","metadata":{"id":"yHRSTQ9-YxbC","outputId":"d94358ec-07ad-4474-f405-2367d1829e83","execution":{"iopub.status.busy":"2023-12-02T21:37:50.300932Z","iopub.execute_input":"2023-12-02T21:37:50.301623Z","iopub.status.idle":"2023-12-02T21:37:58.920200Z","shell.execute_reply.started":"2023-12-02T21:37:50.301589Z","shell.execute_reply":"2023-12-02T21:37:58.918936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs_list, loss_list, label='Training_Loss')\nplt.plot(epochs_list, val_loss_list, label='Validation_Loss')\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Plot of loss vs epochs')\nplt.legend()\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-02T21:38:10.118799Z","iopub.execute_input":"2023-12-02T21:38:10.119634Z","iopub.status.idle":"2023-12-02T21:38:10.432830Z","shell.execute_reply.started":"2023-12-02T21:38:10.119599Z","shell.execute_reply":"2023-12-02T21:38:10.431917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs_list, loss_list, marker='.', linestyle='-', color='b')\n\n# Adding labels and title\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Plot of loss vs epochs')\n\n# Adding a legend\nplt.legend()\n\n# Show the plot\nplt.show()","metadata":{"id":"1Sez4XoKoSt9","execution":{"iopub.status.busy":"2023-12-02T21:38:12.258520Z","iopub.execute_input":"2023-12-02T21:38:12.258893Z","iopub.status.idle":"2023-12-02T21:38:12.478195Z","shell.execute_reply.started":"2023-12-02T21:38:12.258864Z","shell.execute_reply":"2023-12-02T21:38:12.477206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"id":"tX9t6_xD6Elf","outputId":"cffdbbfe-ca01-44ba-fb07-1a7c89e58ec1","execution":{"iopub.status.busy":"2023-12-02T21:38:23.568797Z","iopub.execute_input":"2023-12-02T21:38:23.569146Z","iopub.status.idle":"2023-12-02T21:38:23.586286Z","shell.execute_reply.started":"2023-12-02T21:38:23.569120Z","shell.execute_reply":"2023-12-02T21:38:23.585405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = 0\ntotal = 0\nwith torch.no_grad():\n  for i, (images, labels) in enumerate(testloader, 0):\n    images = images.cuda()\n    n_batches, channels, height, width = images.size()\n    image_batch = torch.autograd.Variable(images.view(-1, channels, height, width))\n    labels = tile(labels, 0, 1).cuda()\n    outputs = model(image_batch)\n    correct += compute_score_with_logits(outputs, labels).sum()\n    total += labels.size(0)\n\nprint('Accuracy on test set: %.3f' % (100 * correct / total))","metadata":{"id":"3DzIF0G4jksc","outputId":"c76b1fb2-1c1f-448e-a6b4-e25c74abfb30","execution":{"iopub.status.busy":"2023-12-02T21:38:24.058268Z","iopub.execute_input":"2023-12-02T21:38:24.058652Z","iopub.status.idle":"2023-12-02T21:38:24.957530Z","shell.execute_reply.started":"2023-12-02T21:38:24.058619Z","shell.execute_reply":"2023-12-02T21:38:24.956674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradients = None\nactivations = None\n\ndef backward_hook(module, grad_input, grad_output):\n  global gradients\n  gradients = grad_output\n\ndef forward_hook(module, args, output):\n  global activations\n  activations = output\n\nbackward_hook = model.features[-1].register_full_backward_hook(backward_hook, prepend=False)\nforward_hook = model.features[-1].register_forward_hook(forward_hook, prepend=False)","metadata":{"id":"gQeFeMpqkXDD","execution":{"iopub.status.busy":"2023-12-02T21:38:27.031488Z","iopub.execute_input":"2023-12-02T21:38:27.031880Z","iopub.status.idle":"2023-12-02T21:38:27.038039Z","shell.execute_reply.started":"2023-12-02T21:38:27.031850Z","shell.execute_reply":"2023-12-02T21:38:27.037109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_to_test_CAM = [0,3,69,420,666]\n\nfor i in images_to_test_CAM:\n  image, label = data[i]\n  image_to_show = image[1]\n  image_batch = image.cuda().unsqueeze(0)\n  model(image_batch).mean().backward()\n\n  # pool the gradients across the channels\n  pooled_gradients = torch.mean(gradients[0], dim=[0, 2, 3])\n\n  # weight the channels by corresponding gradients\n  for i in range(activations.size()[1]):\n    activations[:, i, :, :] *= pooled_gradients[i]\n\n  # average the channels of the activations\n  heatmap = torch.mean(activations, dim=1).squeeze()\n\n  # relu on top of the heatmap\n  heatmap = F.relu(heatmap)\n\n  # normalize the heatmap\n  heatmap /= torch.max(heatmap)\n\n  heatmap_cpu = heatmap.cpu()\n\n  overlay = to_pil_image(heatmap_cpu.detach(), mode='F').resize((300,300), resample=PIL.Image.BICUBIC)\n  cmap = colormaps['jet']\n  overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n\n  fig = plt.figure(figsize=(12, 6))\n  plt.subplot(1, 2, 1)\n  plt.imshow(image_to_show, cmap='gray')\n  plt.title('Original X-ray')\n  plt.axis('off')\n\n  plt.subplot(1, 2, 2)\n  plt.imshow(image_to_show, cmap='gray')\n  plt.title('CAM overlayed')\n  plt.axis('off')\n\n  # Overlaying the third image on the second one\n  plt.imshow(overlay, alpha=0.4, cmap='jet', interpolation='nearest')\n  plt.title('CAM overlayed')\n  plt.axis('off')\n\n  # Show the plot\n  plt.show()\n  diseases = []\n  for i, disease in enumerate(DataPreprocessing.classEncoding.keys()):\n      if label[i] == 1:\n          diseases.append(disease)\n\n  print(f\"Label for the above image:\", diseases)","metadata":{"id":"RKNy_7RjjVxf","outputId":"2a8631ec-516e-4c67-b44d-642b338de78c","execution":{"iopub.status.busy":"2023-12-02T21:38:28.610352Z","iopub.execute_input":"2023-12-02T21:38:28.610737Z","iopub.status.idle":"2023-12-02T21:38:31.140290Z","shell.execute_reply.started":"2023-12-02T21:38:28.610703Z","shell.execute_reply":"2023-12-02T21:38:31.139001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References:\n- https://www.youtube.com/watch?v=fR_0o25kigM&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=20\n- https://github.com/thibaultwillmann/CheXNet-Pytorch\n\n# Runs published:\n- https://www.kaggle.com/code/armeddinosaur/efficientnextv2\n- https://www.kaggle.com/armeddinosaur/efficientnextv2-small","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}